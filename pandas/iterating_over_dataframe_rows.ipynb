{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to iterate over DataFrame rows (and should you?)\n",
    "One of the most searched for (and discussed) questions about pandas is how to iterate over rows in a ```DataFrame```. Often this question comes up right away for new users who have loaded some data into a ```DataFrame``` and now want to do something useful with it. The natural way for most programmers to think of what to do next is to build a loop. They may not understand the \"correct\" way to work with ```DataFrames``` yet, but even experienced developers will often consider iterating over the rows of a ```DataFrame``` to solve a problem. Instead of trying to find the one right answer, it makes better sense to understand the issues involved and know when to choose the best solution.\n",
    "\n",
    "As of this writing, the [top voted question tagged with 'pandas' on Stack Overflow](https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas) is about how to iterate over ```DataFrame``` rows. [It also turns out](https://stackoverflow.blog/2021/04/19/how-often-do-people-actually-copy-and-paste-from-stack-overflow-now-we-know/) that question has [the most copied answer with a code block](https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas/16476974#16476974) on the entire site. The Stack Overflow developers say thousands of people view the answer weekly and copy it to solve their problem.\n",
    "\n",
    "It also turns out that there are potential serious issues with iterating over ```DataFrame``` rows using the top solution. Other answers to the question do a fairly good job of giving other options, but the entire list of 26 (and counting) solutions is extremely confusing. However, there's a better way to think about this question. Instead of asking *how* to iterate over ```DataFrame``` rows, it makes more sense to understand what the options are that are available to you, what their advantages and disadvantages are, and then choose the one that makes sense for you. In some cases, iteration might be the best choice! \n",
    "\n",
    "## But I have heard that iteration is wrong, is that true?\n",
    "First, choosing to iterate over the rows of a ```DataFrame``` is not automatically the wrong way to solve a problem. However, in most cases what beginners are trying to do with iteration is better done with another approach. However, no one should ever feel bad about writing a first solution that uses iteration instead of a better way. That's often the best way to learn, you can think of the iteration solution as the first draft of your essay, you can improve it with some editing.\n",
    "\n",
    "## Now what do we want to do?\n",
    "Let's start with basic questions. If we look at the original question on Stack Overflow, the question and answer just print the content of the ```DataFrame```. First, off, let's all agree that this is not a good way to look at the content of a ```DataFrame```. The standard rendering of a ```DataFrame``` , whether it is rendered with ```print``` or viewed with a Jupyter notebook using ```display``` or as an output in a cell will be far better than what would be printed using custom formatting.\n",
    "\n",
    "If the ```DataFrame``` is large, only some columns and rows may be visible by default. Use ```head``` and ```tail``` to get a sense of the data. If you want to only look at subsets of a ```DataFrame```, instead of using a loop to only display those rows, use the [powerful indexing capabilities of pandas](https://www.wrighters.io/indexing-and-selecting-in-pandas-part-1/).  With a little practice, you can select any combinations of rows or columns to show. Start there first.\n",
    "\n",
    "Now instead of a trivial printing example, let's look at ways to actually use the data for a row in a ```DataFrame```, and look at some ways to solve a problem per row.\n",
    "\n",
    "## Example\n",
    "Let's build an example ```DataFrame```, then think of a few things we may want to work with it. I'll do this by making some fake data (using [Faker](https://faker.readthedocs.io/en/master/)). Note that the columns are different data types (we have some string objects, an integer, and dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_name            object\n",
       "last_name             object\n",
       "start_date    datetime64[ns]\n",
       "end_date      datetime64[ns]\n",
       "city                  object\n",
       "state                 object\n",
       "zipcode               object\n",
       "balance                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "today = datetime.now()\n",
    "next_month = today + timedelta(days=30)\n",
    "df = pd.DataFrame([[fake.first_name(), fake.last_name(),\n",
    "                    fake.date_this_decade(), fake.date_between_dates(today, next_month),\n",
    "                    fake.city(), fake.state(), fake.zipcode(), fake.random_int(-100,1000)]\n",
    "                  for r in range(100)],\n",
    "                  columns=['first_name', 'last_name', 'start_date', 'end_date', 'city', 'state', 'zipcode', 'balance'])\n",
    "\n",
    "\n",
    "df['start_date'] = pd.to_datetime(df['start_date']) # convert to datetimes\n",
    "df['end_date'] = pd.to_datetime(df['end_date'])\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Katherine</td>\n",
       "      <td>Moody</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>2021-06-28</td>\n",
       "      <td>Longberg</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>20496</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>Merritt</td>\n",
       "      <td>2021-03-02</td>\n",
       "      <td>2021-05-30</td>\n",
       "      <td>South Maryborough</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>18495</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Karen</td>\n",
       "      <td>Hensley</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>2021-06-23</td>\n",
       "      <td>Brentside</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>63702</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David</td>\n",
       "      <td>Ferguson</td>\n",
       "      <td>2020-02-02</td>\n",
       "      <td>2021-06-14</td>\n",
       "      <td>Judithport</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>66787</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phillip</td>\n",
       "      <td>Davis</td>\n",
       "      <td>2020-07-17</td>\n",
       "      <td>2021-06-04</td>\n",
       "      <td>Louisberg</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>98616</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name last_name start_date   end_date               city      state  \\\n",
       "0  Katherine     Moody 2020-02-04 2021-06-28           Longberg   Maryland   \n",
       "1      Sarah   Merritt 2021-03-02 2021-05-30  South Maryborough  Tennessee   \n",
       "2      Karen   Hensley 2020-02-29 2021-06-23          Brentside   Missouri   \n",
       "3      David  Ferguson 2020-02-02 2021-06-14         Judithport   Virginia   \n",
       "4    Phillip     Davis 2020-07-17 2021-06-04          Louisberg  Minnesota   \n",
       "\n",
       "  zipcode  balance  \n",
       "0   20496      493  \n",
       "1   18495      680  \n",
       "2   63702      427  \n",
       "3   66787      587  \n",
       "4   98616      211  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first attempt\n",
    "Let's say that our ```DataFrame``` contains customer data. We will also say we have a scoring function for customers that uses multiple customer attributes to give them a score between 'A' and 'F'. Any customer with a negative balance is scored an 'F', above 500 is an 'A', and after that, logic depends on if a customer is a 'legacy' customer and what state they live in.\n",
    "\n",
    "Note that I made doctests for this method, see [my post on Jupyter unit testing](https://www.wrighters.io/unit-testing-python-code-in-jupyter-notebooks/) for more details on how to unit test in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Customer:\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    start_date: datetime\n",
    "    end_date: datetime\n",
    "    city: str\n",
    "    state: str\n",
    "    zipcode: str\n",
    "    balance: int\n",
    "        \n",
    "\n",
    "def score_customer(customer:Customer) -> str:\n",
    "    \"\"\"Give a customer a credit score.\n",
    "    >>> score_customer(Customer(\"Joe\", \"Smith\", datetime(2020, 1, 1), datetime(2023,1,1), \"Chicago\", \"Illinois\", 66666, -5))\n",
    "    'F'\n",
    "    >>> score_customer(Customer(\"Joe\", \"Smith\", datetime(2020, 1, 1), datetime(2023,1,1), \"Chicago\", \"Illinois\", 66666, 50))\n",
    "    'C'\n",
    "    >>> score_customer(Customer(\"Joe\", \"Smith\", datetime(2021, 1, 1), datetime(2023,1,1), \"Chicago\", \"Illinois\", 66666, 50))\n",
    "    'D'\n",
    "    >>> score_customer(Customer(\"Joe\", \"Smith\", datetime(2021, 1, 1), datetime(2023,1,1), \"Chicago\", \"Illinois\", 66666, 150))\n",
    "    'C'\n",
    "    >>> score_customer(Customer(\"Joe\", \"Smith\", datetime(2021, 1, 1), datetime(2023,1,1), \"Chicago\", \"Illinois\", 66666, 250))\n",
    "    'B'\n",
    "    >>> score_customer(Customer(\"Joe\", \"Smith\", datetime(2021, 1, 1), datetime(2023,1,1), \"Chicago\", \"Illinois\", 66666, 350))\n",
    "    'B'\n",
    "    >>> score_customer(Customer(\"Joe\", \"Smith\", datetime(2021, 1, 1), datetime(2023,1,1), \"Santa Fe\", \"California\", 88888, 350))\n",
    "    'A'\n",
    "    >>> score_customer(Customer(\"Joe\", \"Smith\", datetime(2020, 1, 1), datetime(2023,1,1), \"Santa Fe\", \"California\", 88888, 50))\n",
    "    'C'\n",
    "    \"\"\"\n",
    "    if customer.balance < 0:\n",
    "        return 'F'\n",
    "    if customer.balance > 500:\n",
    "        return 'A'\n",
    "    # legacy vs. non-legacy\n",
    "    if customer.start_date > datetime(2020, 1, 1):\n",
    "        if customer.balance < 100:\n",
    "            return 'D'\n",
    "        elif customer.balance < 200:\n",
    "            return 'C'\n",
    "        elif customer.balance < 300:\n",
    "            return 'B'\n",
    "        else:\n",
    "            if customer.state in ['Illinois', 'Indiana']:\n",
    "                return 'B'\n",
    "            else:\n",
    "                return 'A'\n",
    "    else:\n",
    "        if customer.balance < 100:\n",
    "            return 'C'\n",
    "        else:\n",
    "            return 'A'\n",
    "        \n",
    "        \n",
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring our customers\n",
    "OK, now that we have a concrete example, how do we get the score for all of our customers? Let's just go straight to the top answer from the Stack Overflow question, ```DataFrame.iterrows```.  This is a generator that returns the index for a row along with the row as a ```Series```. If you aren't familiar with what a [generator](https://wiki.python.org/moin/Generators) is, you can think of it as a function you can iterate over. As a result, calling ```next``` on it will yield the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " first_name              Katherine\n",
       " last_name                   Moody\n",
       " start_date    2020-02-04 00:00:00\n",
       " end_date      2021-06-28 00:00:00\n",
       " city                     Longberg\n",
       " state                    Maryland\n",
       " zipcode                     20496\n",
       " balance                       493\n",
       " Name: 0, dtype: object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(df.iterrows())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks really promising!  This is a tuple of the index of the item and the item itself, and maybe we can just pass it right into our function. Let's try that out and see what happens. Even though the row is a ```Series```, the columns are the same as the attributes of our ```Customer``` class, so we might be able to just pass this into our scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_customer(next(df.iterrows())[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that seemed to work. Can we just score the entire table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'] = [score_customer(c[1]) for c in df.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this our best choice?\n",
    "Wow, that seems too easy. You can see why this is the top voted answer, since it seems to do exactly what we want. Why would there be any controversy about this answer?\n",
    "\n",
    "As is usually the case with pandas (and really with any software engineering question), picking an ideal solution depends on the inputs. You could read through all the other answers, or just follow along as I attempt to summarize the issues. If the issues don't fit your specific use case, iteration using ```iterrows``` may be a perfectly acceptable solution! I won't judge you. I use it plenty of times, and will summarize at the end how to think about the possible solutions.\n",
    "\n",
    "The arguments against using ```iterrows``` can be grouped into the following categories.\n",
    " 1. Efficiency (Speed and Memory)\n",
    " 1. Mixed types in a row causing issues\n",
    " 1. Readability and maintainability\n",
    "\n",
    "## Speed and Memory\n",
    "In general, if you want things to be fast in pandas (or Numpy, or any framework that offers vectorized calculations), you will not want to iterate through elements but instead choose a vectorized solution. However, even if the solution *can* be vectorized, it might be a lot of work for the programmer to do so. Other answers to the question on Stack Overflow present a host of other solutions. They mostly all fall into one of the following categories, in the following order of preference for speed:\n",
    "1. Vectorization\n",
    "1. Cython routines\n",
    "1. List Comprehensions (vanilla for loop)\n",
    "1. DataFrame.apply()\n",
    "1. DataFrame.itertuples() and iteritems()\n",
    "1. DataFrame.iterrows()\n",
    "\n",
    "### Vectorization\n",
    "The main problem with always telling people to vectorize everything is that at times a vectorized solution may be a real chore to write, debug, and maintain. The examples given to prove that vectorization is preferred often show trivial operations, like simple multiplication. But since the example I started with in this article is not just a single calculation, I decided to write one possible vectorized solution to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_score(df):\n",
    "    return np.select([df['balance'] < 0,\n",
    "                      df['balance'] > 500, # technically not needed, would fall through\n",
    "                      ((df['start_date'] > datetime(2020,1,1)) &\n",
    "                       (df['balance'] < 100)),\n",
    "                      ((df['start_date'] > datetime(2020,1,1)) &\n",
    "                       (df['balance'] >= 100) &\n",
    "                       (df['balance'] < 200)),\n",
    "                      ((df['start_date'] > datetime(2020,1,1)) &\n",
    "                       (df['balance'] >= 200) &\n",
    "                       (df['balance'] < 300)),\n",
    "                      ((df['start_date'] > datetime(2020,1,1)) &\n",
    "                       (df['balance'] >= 300) &\n",
    "                       df['state'].isin(['Illinois', 'Indiana'])),\n",
    "                      ((df['start_date'] >= datetime(2020,1,1)) &\n",
    "                       (df['balance'] < 100)),\n",
    "                     ], # conditions\n",
    "                     ['F',\n",
    "                      'A',\n",
    "                      'D',\n",
    "                      'C',\n",
    "                      'B',\n",
    "                      'B',\n",
    "                      'C'], # choices\n",
    "                     'A') # default score\n",
    "\n",
    "\n",
    "assert (df['score'] == vectorized_score(df)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's more than one way to do this, of course. I chose to use ```np.select``` (you can read more about it and other various ways to update ```DataFrame```s in [my article on using ```where``` and ```mask```](https://www.wrighters.io/selecting-in-pandas-using-where-and-mask/).) I sort of like using ```np.select``` when you have multiple conditions like this, although it's not extremely readable. We could have also done this in vectorized updates and made it much more readable, and it would probably be similar in speed.\n",
    "\n",
    "I personally find this very unreadable, but maybe with some creative comments, it could be clearly explained to future maintainers (or my future self). But the reason we are doing vectorized code is to make this faster. How does performance look for our sample ```DataFrame```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.75 ms ± 489 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vectorized_score(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5 ms ± 911 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [score_customer(c[1]) for c in df.iterrows()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we're almost 5x faster. With our tiny dataset, this probably wouldn't be enough to matter, but with big datasets a simple rewrite to get that much of a speedup makes sense. And I'm sure that a faster vectorized version could be written with a little thought and profiling applied to the situation. But hold on until the end to see what the performance looks like for larger datasets.\n",
    "\n",
    "### Cython\n",
    "[Cython](https://cython.readthedocs.io/en/latest/) is a project that makes it easy to write C extensions for Python using (mostly) Python syntax. I confess that I'm far from a Cython expert, but have found that even just a little bit of effort in Cython can make a Python code hotspot much faster. In this case, we can make a vectorized solution, so using Cython in a non-vectorized solution would not be worth pursuing. However, I did write a simple Cython version [here](https://github.com/wrighter/python_blogposts/blob/main/pandas/iter.pyx) and it was the fastest of the non-vectorized solutions at smaller sized inputs, even with just a tiny bit of effort. Especially for cases where there is a lot of calculation done per row that can't be vectorized, using Cython might be a great choice, but will require an investment in time.\n",
    "\n",
    "### List comprehensions\n",
    "Now the next option is a little different. I admit that I don't think I've used this technique often. The idea here is to use a list comprehension, invoking your function with each element in your ```DataFrame```. Note that I did use a list comprehension already, but with ```iterrows```.  This time instead of using ```iterrows```, the data is pulled out of each column in the ```DataFrame```. If your function has multiple arguments, you can use  ```zip``` to make tuples of the arguments, passing in the columns in your ```DataFrame``` to match the argument order. Now to do this, I'll need a modified scoring function, since I don't have already constructed ```Customer``` objects in my ```DataFrame```, and creating them just to invoke the function would add another layer. I only use three attributes of the customer, so here's a simple rewrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_customer_attributes(balance:int, start_date:datetime, state:str) -> str:\n",
    "    if balance < 0:\n",
    "        return 'F'\n",
    "    if balance > 500:\n",
    "        return 'A'\n",
    "    # legacy vs. non-legacy\n",
    "    if start_date > datetime(2020, 1, 1):\n",
    "        if balance < 100:\n",
    "            return 'D'\n",
    "        elif balance < 200:\n",
    "            return 'C'\n",
    "        elif balance < 300:\n",
    "            return 'B'\n",
    "        else:\n",
    "            if state in ['Illinois', 'Indiana']:\n",
    "                return 'B'\n",
    "            else:\n",
    "                return 'A'\n",
    "    else:\n",
    "        if balance < 100:\n",
    "            return 'C'\n",
    "        else:\n",
    "            return 'A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's what the first loop of the list comprehension will use to call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493, Timestamp('2020-02-04 00:00:00'), 'Maryland')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(zip(df['balance'], df['start_date'], df['state']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score3'] = [score_customer_attributes(*a) for a in zip(df['balance'], df['start_date'], df['state'])]\n",
    "assert (df['score'] == df['score3']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how fast is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 µs ± 11.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [score_customer_attributes(*a) for a in zip(df['balance'], df['start_date'], df['state'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's much faster, over 70x faster than the original. By just taking the raw data and invoking a simple Python function, the scores are all calculated quickly in Python space. No row conversions to ```Series``` need to take place.\n",
    "\n",
    "Note that we could also invoke our original function, we'd just have to make a ```Customer``` object to pass in. This is a bit uglier, and still quite fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 µs ± 2.59 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [score_customer(Customer(first_name='', last_name='', end_date=None, city=None, zipcode=None, balance=a[0], start_date=a[1], state=a[2])) for a in zip(df['balance'], df['start_date'], df['state'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.apply\n",
    "We can also use ```DataFrame.apply```. Note that to apply this to rows, you need to pass in the correct axis since it defaults to the rows. The axis argument here is telling apply which index you want to have in the object passed to your function. We want each object to a customer row, with the columns as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.apply(score_customer, axis=1) == df['score']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.57 ms ± 117 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df.apply(score_customer, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance here is better than our original, over 3x faster. This is also very readable, and allows us to use our easy to read and maintain original function. It's still slower than the list comprehension though because it is constructing a ```Series``` object for each row. \n",
    "\n",
    "## DataFrame.iteritems and DataFrame.itertuples\n",
    "Now we look at the regular iteration methods in more detail. There are three ```iter``` functions available for ```DataFrame```s, ```iteritems```, ```itertuples```, and ```iterrows```. ```DataFrames``` also support iteration directly, but these functions don't all iterate over the same things. Since what all these methods do can be really confusing, let's just review them all here.\n",
    "\n",
    "1. ```iter(df)``` (calls the ```DataFrame.__iter__``` method). Iterate over the info axis, which for ```DataFrames``` is the column names, not the values.\n",
    "1. ```iteritems```. Iterate over the columns, returning a tuple of column name and the column as a ```Series```.\n",
    "1. ```items```. This is the same as above. ```iteritems``` actually just invokes ```items```.\n",
    "1. ```iterrows```. We already have seen this, iterates through the rows, but returns them as a tuple of index and the row, as a ```Series```.\n",
    "1. ```itertuples```. Iterates over the rows, returning a ```namedtuple``` for each row. You can optionally change the name of the tuple and disable the index being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first_name'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(df)) # 'first_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('first_name',\n",
       " 0       Katherine\n",
       " 1           Sarah\n",
       " 2           Karen\n",
       " 3           David\n",
       " 4         Phillip\n",
       "          ...     \n",
       " 95         Robert\n",
       " 96    Christopher\n",
       " 97        Kristen\n",
       " 98       Nicholas\n",
       " 99       Caroline\n",
       " Name: first_name, Length: 100, dtype: object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(df.iteritems())\n",
    "next(df.items())       # these two are equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " first_name              Katherine\n",
       " last_name                   Moody\n",
       " start_date    2020-02-04 00:00:00\n",
       " end_date      2021-06-28 00:00:00\n",
       " city                     Longberg\n",
       " state                    Maryland\n",
       " zipcode                     20496\n",
       " balance                       493\n",
       " score                           A\n",
       " score3                          A\n",
       " Name: 0, dtype: object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(df.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas(Index=0, first_name='Katherine', last_name='Moody', start_date=Timestamp('2020-02-04 00:00:00'), end_date=Timestamp('2021-06-28 00:00:00'), city='Longberg', state='Maryland', zipcode='20496', balance=493, score='A', score3='A')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(df.itertuples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using itertuples\n",
    "Since we already looked at ```iterrows```, we only need to look at ```itertuples```.  As you can see, the returned value, a ```namedtuple```, can be used in our original function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ([score_customer(c[1]) for c in df.iterrows()]  == df['score']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858 µs ± 5.23 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [score_customer(t) for t in df.itertuples()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance here is pretty good, over 12x faster. The construction of a ```namedtuple``` for each row is much faster than construction of a ```Series```.\n",
    "\n",
    "## Mixed types in a row\n",
    "Now is a good time to bring up another difference between ```iterrows``` and ```itertuples```. A ```namedtuple``` can properly represent any type in a single row. In our case, we have strings, date types, and integers. A pandas ```Series```, however, has to have only one datatype for the entire ```Series```. Because our datatypes were diverse enough, they were all represented as ```object``` types, and ended up retaining their type, with no functionality issues for us. But this is not always the case!\n",
    "\n",
    "If your columns have different numerical types, for example, they will end up being the type that can represent all of them. This can cause your data returned by ```itertuples``` and ```iterrows``` to be slightly different between these two methods, so watch out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "integer_column      int64\n",
       "float_column      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmixed = pd.DataFrame({'integer_column': [1,2,3], 'float_column': [1.1, 2.2, 3.3]})\n",
    "dfmixed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas(Index=0, integer_column=1, float_column=1.1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dfmixed.itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " integer_column    1.0\n",
       " float_column      1.1\n",
       " Name: 0, dtype: float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dfmixed.iterrows())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other choices\n",
    "There are other options for iteration, of course. For example, you could increment an integer offset and use the ```iloc``` indexer on the ```DataFrame``` to select any row. Of course, this is really no different from any other iteration, while also being non-idiomatic so others reading your code will probably find it hard to read and understand. I built a naive version of this in the performance comparison code for the summary below, if you want to see it.\n",
    "\n",
    "## Choosing well\n",
    "Choosing the right solution depends on essentially two factors:\n",
    "1. How big is your data set?\n",
    "1. What can you write (and maintain) easily?\n",
    "\n",
    "    See below the running time for a variety of solutions [the code to generate this is here](https://github.com/wrighter/python_blogposts/blob/main/pandas/iter.py). As you can see, only the vectorized solution holds up well with larger data. If your data set is huge, vectorized solutions may be your only reasonable choice.\n",
    "\n",
    "However, depending on how many times you need to execute your code, how long it takes you to write it correctly, and how well you can maintain it going forward, you may choose any of the other solutions and be fine. In fact, they all grow linearly with increasing data for these solutions.\n",
    "\n",
    "Maybe one way to think about this is not just big-O notation, but big-U notation. In other words, how long will it take you to write a correct solution. If it's less than the running time of your code, an iterative solution may be totally fine. \n",
    "\n",
    "One more point, sometimes writing the iterative solution on a smaller set is easy, and you may do that first, then write the vectorized version. Verify your results with the iterative solution to make sure you did it correctly, then use the vectorized version on the larger full data set.\n",
    "\n",
    "I hope you've found this dive into ```DataFrame``` iteration interesting. I know I learned a few things along the way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

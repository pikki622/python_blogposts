{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be very common when dealing with time series data to end up with duplicate data. This can happen for a variety of reasons, and I've encountered it more than one time and tried different approaches to eliminate the duplicate values. There's a [gem of a solution on Stack Overflow](https://stackoverflow.com/questions/13035764/remove-rows-with-duplicate-indices-pandas-dataframe-and-timeseries#34297689) and I thought it would be helpful to walk through the possible solutions to this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, I'll just work with a ```Series``` of floating point data. This could be anything, but we could pretend it's something that's manually maintained, like an earnings estimate for a stock, or a temperature reading, or a sales for a store on a given date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020-01-01    78.535174\n",
       "2020-01-02    66.960187\n",
       "2020-01-03    14.711618\n",
       "2020-01-04    90.651161\n",
       "2020-01-05     3.127869\n",
       "2020-01-06    40.417499\n",
       "2020-01-07    46.791960\n",
       "2020-01-08    40.818973\n",
       "2020-01-09    85.778448\n",
       "2020-01-10    73.401114\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "items = pd.Series(np.random.random_sample(10) * 100, pd.date_range('2020-01-01', periods=10))\n",
    "\n",
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have 10 periods of data, and the index (a ```DatetimeIndex``` with 10 days) all have unique values. But let's say in our data, corrected data appears in the same source file. I'll do something a bit contrived here and concatenate two ```Series``` that have some of the same dates in them, but in real life you can imagine a number of ways that data will show up in your sources with duplicated data for the same time stamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = pd.Series(np.random.random_sample(3) * 75, pd.date_range('2020-01-04', periods=3))\n",
    "\n",
    "combined = pd.concat([items, corrected])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how do we get rid of this duplicated data? Let's say that we want to only keep the most recent data in our file, assuming that it was a correction or updated value that we prefer to use. Instead of going right to the accepted solution on Stack Overflow, I'm going to work through the pandas documentation to see what the possible solutions are, and hopefully end up in the same place!\n",
    "\n",
    "First, let's see if we can answer the question of whether our data has duplicate items in the index. In the pandas docs, we see a few promising methods, including a [```duplicated```](https://pandas.pydata.org/docs/reference/api/pandas.Index.duplicated.html) method, and also a ```has_duplicates``` property. Let's see if those report what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.index.has_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the methods available to look at are ```duplicated``` and ```drop_duplicates```. For ```duplicated```, the method will return an array of boolean values, where ```True``` indicates the duplicate. You can use the ```keep``` argument to keep either the first (default) or last occurrence of the value in your index. In ```drop_duplicates```, you get an ```Index``` returned with the duplicates already removed, and you can pass in the same ```keep``` argument with the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True,  True,  True, False, False, False,\n",
       "       False, False, False, False])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.index.duplicated(keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-07',\n",
       "               '2020-01-08', '2020-01-09', '2020-01-10', '2020-01-04',\n",
       "               '2020-01-05', '2020-01-06'],\n",
       "              dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.index.drop_duplicates(keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so what do we do now with these two options? The first boolean array can be used to just pick the values that we want to keep, but the ```True``` values are the ones we want to drop. That is pretty easy, just invert it with a ```~```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~combined.index.duplicated(keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That can be used to select the values you want out of the array, and gets us to a good solution. We need to sort the index since it is not in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020-01-01    32.709476\n",
       "2020-01-02    60.135948\n",
       "2020-01-03    63.326407\n",
       "2020-01-04    40.548428\n",
       "2020-01-05     1.234698\n",
       "2020-01-06    23.512759\n",
       "2020-01-07    56.705172\n",
       "2020-01-08    29.921226\n",
       "2020-01-09    73.158245\n",
       "2020-01-10    68.840243\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[~combined.index.duplicated(keep='last')].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to use the second method, ```drop_duplicates```, we need to find a way to use that to grab the values out of our ```Series``` that we want to keep. This is a bit more complicated. First, we can use the ```reset_index``` method which is a handy way to take the index (in our case a ```DatetimeIndex```) and turn it into a column on a ```DataFrame``` momentarily with a new regular, non-repeating index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>32.709476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>60.135948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>63.326407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>49.435518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>75.190352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>11.834493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>56.705172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>29.921226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>73.158245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>68.840243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>40.548428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>1.234698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>23.512759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        index          0\n",
       "0  2020-01-01  32.709476\n",
       "1  2020-01-02  60.135948\n",
       "2  2020-01-03  63.326407\n",
       "3  2020-01-04  49.435518\n",
       "4  2020-01-05  75.190352\n",
       "5  2020-01-06  11.834493\n",
       "6  2020-01-07  56.705172\n",
       "7  2020-01-08  29.921226\n",
       "8  2020-01-09  73.158245\n",
       "9  2020-01-10  68.840243\n",
       "10 2020-01-04  40.548428\n",
       "11 2020-01-05   1.234698\n",
       "12 2020-01-06  23.512759"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use ```drop_duplicates```, but we'll use the ```DataFrame``` version which adds a ```subset``` argument that can be used to only consider a certain column (our new 'index' column) for duplicates to drop. Now since this is now a ```DataFrame``` and not a ```Series```, we will reset the index to our index column using ```set_index``` and return the column ```0```. This gives us the same result as the earlier method, but in a much more roundabout way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "2020-01-01    32.709476\n",
       "2020-01-02    60.135948\n",
       "2020-01-03    63.326407\n",
       "2020-01-04    40.548428\n",
       "2020-01-05     1.234698\n",
       "2020-01-06    23.512759\n",
       "2020-01-07    56.705172\n",
       "2020-01-08    29.921226\n",
       "2020-01-09    73.158245\n",
       "2020-01-10    68.840243\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.reset_index().drop_duplicates(subset='index', keep='last').set_index('index')[0].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other way to do this is to use ```groupby``` and a grouping function (in this case the ```last```) to select the values we want. This method provides us with sorted output and also looks simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020-01-01    32.709476\n",
       "2020-01-02    60.135948\n",
       "2020-01-03    63.326407\n",
       "2020-01-04    40.548428\n",
       "2020-01-05     1.234698\n",
       "2020-01-06    23.512759\n",
       "2020-01-07    56.705172\n",
       "2020-01-08    29.921226\n",
       "2020-01-09    73.158245\n",
       "2020-01-10    68.840243\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.groupby(combined.index).last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the best way to do this? Like the question on Stack Overflow, I prefer the first method for readability, but the last is also pretty simple. One good argument for choosing the first method is speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271 µs ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit combined[~combined.index.duplicated(keep='last')].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6 ms ± 62.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit combined.reset_index().drop_duplicates(subset='index', keep='last').set_index('index')[0].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595 µs ± 7.57 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit combined.groupby(combined.index).last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, after digging through all that, I hope you understand a bit more about how to remove duplicate items from a ```Series``` or ```DataFrame``` and why some methods might be better to choose than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
